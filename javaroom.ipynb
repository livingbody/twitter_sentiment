{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、基于PaddleNLP skep_ernie_2.0_large_en模型的英文twitter 情感分析\n",
    "本文收集31962条twitter数据，划分为train、dev，其中情感氛围正向、负向两种。\n",
    "\n",
    ">众所周知，我们前期通过PaddleNLP课程学习了NLP相关知识，情感分析方面，在中文情感数据集进行了大量的联系，在此我再英文twitter上进行情感分析。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/4d342c89012b429e9fc31428f7b704c65e8ba515d0384937914143dff4d3aef5)\n",
    "\n",
    "* 正向： 表示正面积极的情感，如高兴，幸福，惊喜，期待等。\n",
    "* 负向： 表示负面消极的情感，如难过，伤心，愤怒，惊恐等。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.升级PaddleNLP到2.0.3版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31943,0,this week is flying by   #humpday - #wednesday #kamp #ucsdâ¦ \r\n",
      "31944,0, @user modeling photoshoot this friday yay #model #me #follow #emo   \r\n",
      "31945,0,\"you're surrounded by people who love you (even more than you deserve) and yet, why are so hateful?   \"\r\n",
      "31946,0,feel like... ðð¶ð #dog #summer #hot #help #sun #day   #more \r\n",
      "31947,1,@user omfg i'm offended! i'm a  mailbox and i'm proud! #mailboxpride  #liberalisme\r\n",
      "31948,1,@user @user you don't have the balls to hashtag me as a  but you say i am to weasel away.. lumpy tony.. dipshit.\r\n",
      "31949,1,\" makes you ask yourself, who am i? then am i anybody? until ....god . oh thank you god!\"\r\n",
      "31950,0,hear one of my new songs! don't go - katie ellie #youtube #original #music #song #relationship #songwriter    \r\n",
      "31951,0,\" @user you can try to 'tail' us to stop, 'butt' we're just having too good of a time!  #goldenretriever   #animals \"\r\n",
      "31952,0,i've just posted a new blog: #secondlife #lonely #neko   \r\n",
      "31953,0,@user you went too far with @user  \r\n",
      "31954,0,good morning #instagram #shower #water #berlin #berlincitygirl   #girl #newyork #zÃ¼rich #genf #bern \r\n",
      "31955,0,#holiday   bull up: you will dominate your bull and you will direct it whatever you want it to do. when you \r\n",
      "31956,0,less than 2 weeks ð",
      "ðð¼ð¹ððµ @user #ibiza#bringiton#mallorca#holidays#summer  \r\n",
      "31957,0,off fishing tomorrow @user carnt wait first time in 2 years  \r\n",
      "31958,0,ate @user isz that youuu?ðððððððððâ¤ï¸ \r\n",
      "31959,0,  to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm. #shame #imwithher\r\n",
      "31960,0,listening to sad songs on a monday morning otw to work is sad  \r\n",
      "31961,1,\"@user #sikh #temple vandalised in in #calgary, #wso condemns  act  \"\r\n",
      "31962,0,thank you @user for you follow  \r\n"
     ]
    }
   ],
   "source": [
    "!tail -n20 data/data96052/Practice-Twitter-Sentiment-Analysis.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.自定义PaddleNLP dataset的read方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddle.io import Dataset, Subset\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "\r\n",
    "def read(data_path):\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\r\n",
    "        next(f)\r\n",
    "        for line in f:\r\n",
    "            a=line.find(',')\r\n",
    "            b=line.find(',', a+1)\r\n",
    "            ids = line.strip('\\n')[:a]\r\n",
    "            labels= line.strip('\\n')[a+1:b]\r\n",
    "            sentences = line.strip('\\n')[b+1:]            \r\n",
    "            yield {'text': sentences, 'label': labels, 'qid':ids}\r\n",
    "\r\n",
    "\r\n",
    "# data_path为read()方法的参数\r\n",
    "dataset_ds = load_dataset(read, data_path='data/data96052/Practice-Twitter-Sentiment-Analysis.csv',lazy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.使用paddle.io.Subset把数据集进行划分train、dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25569\n",
      "6393\n"
     ]
    }
   ],
   "source": [
    "# 在这进行划分\n",
    "train_ds = Subset(dataset=dataset_ds, indices=[i for i in range(len(dataset_ds)) if i % 5 != 1])\n",
    "dev_ds = Subset(dataset=dataset_ds, indices=[i for i in range(len(dataset_ds)) if i % 5 == 1])\n",
    "# 在转换为MapDataset类型\n",
    "train_ds = MapDataset(train_ds)\n",
    "dev_ds = MapDataset(dev_ds)\n",
    "print(len(train_ds))\n",
    "print(len(dev_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、SKEP模型加载\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fc21e1201154451a80f32e0daa5fa84386c1b12e4b3244e387ae0b177c1dc963)\n",
    "\n",
    "目前该模型有中英文3个预训练模型，选取英文skep_ernie_2.0_large_en预训练模型\n",
    "```\n",
    "        \"model_state\": {\n",
    "            \"skep_ernie_1.0_large_ch\":\n",
    "            \"https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams\",\n",
    "            \"skep_ernie_2.0_large_en\":\n",
    "            \"https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_2.0_large_en.pdparams\",\n",
    "            \"skep_roberta_large_en\":\n",
    "            \"https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_roberta_large_en.pdparams\",\n",
    "```\n",
    "* SkepForSequenceClassification可用于句子级情感分析和目标级情感分析任务。其通过预训练模型SKEP获取输入文本的表示，之后将文本表示进行分类。\n",
    "* pretrained_model_name_or_path：模型名称。支持\"skep_ernie_1.0_large_ch\"，\"skep_ernie_2.0_large_en\"。\n",
    "* \"skep_ernie_1.0_large_ch\"：是SKEP模型在预训练ernie_1.0_large_ch基础之上在海量中文数据上继续预训练得到的中文预训练模型；\n",
    "* \"skep_ernie_2.0_large_en\"：是SKEP模型在预训练ernie_2.0_large_en基础之上在海量英文数据上继续预训练得到的英文预训练模型；\n",
    "* num_classes: 数据集分类类别数。\n",
    "\n",
    "关于SKEP模型实现详细信息参考：https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/skep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\r\n",
    "\r\n",
    "# 指定模型名称，一键加载模型\r\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\", num_classes=2)\r\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\r\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、NLP数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 加入日志显示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from visualdl import LogWriter\r\n",
    "\r\n",
    "writer = LogWriter(\"./log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.SkepTokenizer数据处理\n",
    "SKEP模型对中文文本处理按照字粒度进行处理，我们可以使用PaddleNLP内置的SkepTokenizer完成一键式处理。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader\r\n",
    "\r\n",
    "def convert_example(example,\r\n",
    "                    tokenizer,\r\n",
    "                    max_seq_length=512,\r\n",
    "                    is_test=False):\r\n",
    "   \r\n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\r\n",
    "    encoded_inputs = tokenizer(\r\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\r\n",
    "\r\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\r\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        # label：情感极性类别\r\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, label\r\n",
    "    else:\r\n",
    "        # qid：每条数据的编号\r\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 批量数据大小\r\n",
    "batch_size = 64\r\n",
    "# 文本序列最大长度\r\n",
    "max_seq_length = 128\r\n",
    "\r\n",
    "# 将数据处理成模型可读入的数据格式\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length)\r\n",
    "\r\n",
    "# 将数据组成批量式数据，如\r\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\r\n",
    "# 将每条数据label堆叠在一起\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack()  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "dev_data_loader = create_dataloader(\r\n",
    "    dev_ds,\r\n",
    "    mode='dev',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  五、模型训练和评估\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。\n",
    "\n",
    "推荐超参设置：\n",
    "\n",
    "* max_seq_length=256\n",
    "* batch_size=48\n",
    "* learning_rate=2e-5\n",
    "* epochs=10\n",
    "实际运行时可以根据显存大小调整batch_size和max_seq_length大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "\r\n",
    "from utils import evaluate\r\n",
    "\r\n",
    "# 训练轮次\r\n",
    "epochs = 3\r\n",
    "# 训练过程中保存模型参数的文件夹\r\n",
    "ckpt_dir = \"skep_ckpt\"\r\n",
    "# len(train_data_loader)一轮训练所需要的step数\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "# Adam优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=2e-5,\r\n",
    "    parameters=model.parameters())\r\n",
    "# 交叉熵损失函数\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "# accuracy评价指标\r\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\r\n",
    "global_step = 0\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "        # 喂数据给model\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        # 计算损失函数值\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        # 预测分类概率值\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        # 计算acc\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0:\r\n",
    "            print(\r\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss, acc,\r\n",
    "                    10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "        \r\n",
    "        # 反向梯度回传，更新参数\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % 100 == 0:\r\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\r\n",
    "            if not os.path.exists(save_dir):\r\n",
    "                os.makedirs(save_dir)\r\n",
    "            # 评估当前训练的模型\r\n",
    "            evaluate(model, criterion, metric, dev_data_loader)\r\n",
    "            # 保存当前模型参数等\r\n",
    "            model.save_pretrained(save_dir)\r\n",
    "            # 保存tokenizer的词表等\r\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "global step 1110, epoch: 3, batch: 310, loss: 0.12702, accu: 0.99219, speed: 0.26 step/s\n",
    "global step 1120, epoch: 3, batch: 320, loss: 0.05266, accu: 0.99297, speed: 1.89 step/s\n",
    "global step 1130, epoch: 3, batch: 330, loss: 0.02344, accu: 0.99323, speed: 1.77 step/s\n",
    "global step 1140, epoch: 3, batch: 340, loss: 0.00321, accu: 0.99375, speed: 1.86 step/s\n",
    "global step 1150, epoch: 3, batch: 350, loss: 0.12621, accu: 0.99344, speed: 1.92 step/s\n",
    "global step 1160, epoch: 3, batch: 360, loss: 0.00110, accu: 0.99349, speed: 1.89 step/s\n",
    "global step 1170, epoch: 3, batch: 370, loss: 0.00077, accu: 0.99353, speed: 1.92 step/s\n",
    "global step 1180, epoch: 3, batch: 380, loss: 0.01631, accu: 0.99336, speed: 1.89 step/s\n",
    "global step 1190, epoch: 3, batch: 390, loss: 0.02353, accu: 0.99306, speed: 1.68 step/s\n",
    "global step 1200, epoch: 3, batch: 400, loss: 0.03405, accu: 0.99309, speed: 1.92 step/s\n",
    "eval loss: 0.07968, accu: 0.97794\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.test数据处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n"
     ]
    }
   ],
   "source": [
    "# 在这进行划分\r\n",
    "test_ds = Subset(dataset=dataset_ds, indices=[i for i in range(len(dataset_ds)) if i % 50 == 1])\r\n",
    "# 在转换为MapDataset类型\r\n",
    "test_ds = MapDataset(test_ds)\r\n",
    "\r\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import paddle\r\n",
    "\r\n",
    "# 处理测试集数据\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length,\r\n",
    "    is_test=True)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\r\n",
    "    Stack() # qid\r\n",
    "): [data for data in fn(samples)]\r\n",
    "test_data_loader = create_dataloader(\r\n",
    "    test_ds,\r\n",
    "    mode='test',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.载入训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from skep_ckpt/model_1200/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\r\n",
    "params_path = 'skep_ckpt/model_1200/model_state.pdparams'\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    # 加载模型参数\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_map = {0: '0', 1: '1'}\r\n",
    "results = []\r\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\r\n",
    "model.eval()\r\n",
    "for batch in test_data_loader:\r\n",
    "    input_ids, token_type_ids, qids = batch\r\n",
    "    # 喂数据给模型\r\n",
    "    logits = model(input_ids, token_type_ids)\r\n",
    "    # 预测分类\r\n",
    "    probs = F.softmax(logits, axis=-1)\r\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "    idx = idx.tolist()\r\n",
    "    labels = [label_map[i] for i in idx]\r\n",
    "    qids = qids.numpy().tolist()\r\n",
    "    results.extend(zip(qids, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_dir = \"./results\"\r\n",
    "if not os.path.exists(res_dir):\r\n",
    "    os.makedirs(res_dir)\r\n",
    "# 写入预测结果\r\n",
    "with open(os.path.join(res_dir, \"Twitter.tsv\"), 'w', encoding=\"utf8\") as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for qid, label in results:\r\n",
    "        f.write(str(qid[0])+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\tprediction\r\n",
      "2\t0\r\n",
      "52\t0\r\n",
      "102\t0\r\n",
      "152\t1\r\n",
      "202\t0\r\n",
      "252\t0\r\n",
      "302\t0\r\n",
      "352\t0\r\n",
      "402\t0\r\n"
     ]
    }
   ],
   "source": [
    "!head  results/Twitter.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
